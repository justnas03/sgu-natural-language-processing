{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice 27/9/2024: Chapter 2 - Regular Expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Basics of Regular Expressions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regex helps in matching and replacing specific parts of a string based on predefined patterns. <br/>\n",
    "**Ex**: define pattern then use re.search(patter, word) to match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['that', 'chat', 'cat', 'phat']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "#Define a regex pattern to match words with \"c\" and \"h\" followed by \"a\" and \"t\"\n",
    "pattern = r\"[ch]at\"\n",
    "\n",
    "words = [\"that\", \"at\", \"chat\", \"cat\", \"fat\", \"phat\"] #words to test\n",
    "\n",
    "matching_words = [word for word in words if re.search(pattern, word)] #matching_words are word in words if that word matches the pattern (using re.search)\n",
    "\n",
    "print(matching_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Regex in text preprocessing \n",
    "<strong>Applications of Regex in Text Preprocessing:</strong> <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Tokenization** <br>\n",
    "The \\w+ regex pattern is used to match consecutive word characters (letters, digits, underscores), extracting tokens using ``re.findall()``. <br>\n",
    "However, shortened words like \"we're\" may be split improperly and require full-form tokenization (e.g., \"we are\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     [The, laptop, s, battery, life, is, outstandin...\n",
      "1     [Terrible, customer, service, My, issue, wasn,...\n",
      "2     [Fast, shipping, but, the, packaging, was, dam...\n",
      "3     [I, love, the, sleek, design, of, the, phone, ...\n",
      "4     [The, software, is, buggy, and, crashes, frequ...\n",
      "5     [The, restaurant, ambiance, was, nice, but, th...\n",
      "6     [Absolutely, fantastic, headphones, Great, sou...\n",
      "7     [The, movie, was, a, complete, waste, of, time...\n",
      "8     [Bought, this, for, my, son, he, enjoys, it, a...\n",
      "9     [The, hotel, staff, was, extremely, friendly, ...\n",
      "10    [Poor, quality, fabric, not, worth, the, price...\n",
      "11    [Had, a, great, time, at, the, amusement, park...\n",
      "12    [The, app, interface, is, intuitive, and, easy...\n",
      "13    [The, concert, was, amazing, but, the, seating...\n",
      "14    [This, book, is, a, masterpiece, full, of, ins...\n",
      "15    [Received, a, defective, product, had, to, ret...\n",
      "16    [The, vacuum, cleaner, is, very, powerful, but...\n",
      "17    [Great, experience, at, the, car, dealership, ...\n",
      "18    [Not, happy, with, the, subscription, service,...\n",
      "19    [The, smartwatch, has, many, features, but, th...\n",
      "Name: tokens, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "df = pd.read_csv(\"reviews.csv\")\n",
    "\n",
    "#Uses 're.findall()' to match all sequences of word characters (letters, digits, and underscores) in the text. \n",
    "#'\\w+' is the regex pattern used to find these sequences, treating them as tokens.\n",
    "def tokenize_text(text):\n",
    "    return re.findall(r'\\w+',text) \n",
    "\n",
    "# Applies the 'tokenize_text' function to each row of the 'review_text' column in the DataFrame, \n",
    "# creating a new column 'tokens' that contains the list of tokens (words) for each review.\n",
    "df['tokens'] = df['review_text'].apply(tokenize_text)\n",
    "\n",
    "\n",
    "print(df['tokens'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Text cleaning and normalization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     the laptop s battery life is outstanding lasti...\n",
      "1      terrible customer service my issue wasn t res...\n",
      "2     fast shipping but the packaging was damaged up...\n",
      "3     i love the sleek design of the phone it s ligh...\n",
      "4     the software is buggy and crashes frequently v...\n",
      "5     the restaurant ambiance was nice but the food ...\n",
      "6     absolutely fantastic headphones great sound qu...\n",
      "7     the movie was a complete waste of time plot wa...\n",
      "8     bought this for my son he enjoys it a lot very...\n",
      "9     the hotel staff was extremely friendly and acc...\n",
      "10    poor quality fabric not worth the price wouldn...\n",
      "11    had a great time at the amusement park the rid...\n",
      "12    the app interface is intuitive and easy to nav...\n",
      "13    the concert was amazing but the seating was to...\n",
      "14    this book is a masterpiece full of insightful ...\n",
      "15    received a defective product had to return it ...\n",
      "16    the vacuum cleaner is very powerful but quite ...\n",
      "17    great experience at the car dealership the sta...\n",
      "18    not happy with the subscription service too ma...\n",
      "19    the smartwatch has many features but the batte...\n",
      "Name: cleaned_text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# First, re.sub('[^0-9a-zA-Z]+', ' ', text.lower()) replaces any character that is not alphanumeric (a-z, A-Z, 0-9) with a space and converts the text to lowercase.\n",
    "\n",
    "# Second, re.sub(' +', ' ', ...) ensures that multiple spaces are reduced to a single space.\n",
    "    \n",
    "def clean_text(text):\n",
    "    cleaned_text = re.sub(' +', ' ', re.sub('[^0-9a-zA-Z]',' ', text.lower()))\n",
    "    return cleaned_text\n",
    "\n",
    "df['cleaned_text'] = df['review_text'].apply(clean_text)\n",
    "\n",
    "print(df['cleaned_text'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Named entity recognition:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'NOUNS': [[], [], [], [], ['software'], ['restaurant'], [], ['movie'], [], [], [], [], [], [], [], [], [], [], [], []], 'PERSON': [[], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], []]}\n"
     ]
    }
   ],
   "source": [
    "patterns = {\n",
    "    'NOUNS' : r'(software|restaurant|movie)',\n",
    "    'PERSON': r'(Barack Obama|John Doe|Jane Smith)'\n",
    "}\n",
    "\n",
    "named_entities = {} \n",
    "for entity, pattern in patterns.items():\n",
    "    def find_entities(text):\n",
    "        return re.findall(pattern, text)\n",
    "    df[entity] = df['review_text'].apply(find_entities)\n",
    "    named_entities[entity] = df[entity].tolist()\n",
    "\n",
    "print(named_entities)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
