{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     The laptops battery life is outstanding lastin...\n",
      "1     Terrible customer service My issue wasnt resol...\n",
      "2     Fast shipping but the packaging was damaged up...\n",
      "3     I love the sleek design of the phone  its ligh...\n",
      "4     The software is buggy  and crashes frequently ...\n",
      "5     The restaurant ambiance was nice  but the food...\n",
      "6     Absolutely fantastic headphones Great sound qu...\n",
      "7     The movie was a complete waste of time  plot w...\n",
      "8     Bought this for my son  he enjoys it a lot ver...\n",
      "9     The hotel staff was extremely friendly  and ac...\n",
      "10    Poor quality fabric not worth the price  Would...\n",
      "11    Had a great time at the amusement park the rid...\n",
      "12    The app interface is intuitive and easy to nav...\n",
      "13    The concert was amazing but the seating was to...\n",
      "14    This book is a masterpiece full of insightful ...\n",
      "15    Received a defective product  had to return it...\n",
      "16    The vacuum cleaner is very powerful but quite ...\n",
      "17    Great experience at the car dealership  the st...\n",
      "18    Not happy with the subscription service  Too m...\n",
      "19    The smartwatch has many features but the batte...\n",
      "Name: clean_text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd  # Import the pandas library for data manipulation and analysis, specifically for reading and handling CSV files.\n",
    "import re  # Import the regular expression (re) library for pattern matching and text manipulation.\n",
    "\n",
    "# Read the CSV file 'reviews.csv' into a Pandas DataFrame. Make sure the CSV file is in the same directory as the script, or provide the correct path.\n",
    "df = pd.read_csv('reviews.csv') \n",
    "\n",
    "# Define a function to remove special characters from a string (text). This function will be applied to the text data.\n",
    "def remove_special_characters(text):\n",
    "    # Use the re.sub() function to replace any character that is not a letter (a-z, A-Z), number (0-9), or whitespace with an empty string.\n",
    "    # This removes punctuation and special characters from the text.\n",
    "    clean_text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "    return clean_text  # Return the cleaned text without special characters.\n",
    "\n",
    "# Apply the remove_special_characters function to the 'review_text' column in the DataFrame.\n",
    "# This creates a new column 'clean_text' containing the processed reviews with special characters removed.\n",
    "df['clean_text'] = df['review_text'].apply(remove_special_characters)\n",
    "\n",
    "# Print the cleaned text column to see the result.\n",
    "print(df['clean_text']) \n",
    "\n",
    "# Command to run the code in the terminal (assuming this script is saved as 'clean_reviews.py'):\n",
    "# python clean_reviews.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the first sentence.\n",
      "This is the second sentence.\n",
      "This is the third sentence.\n"
     ]
    }
   ],
   "source": [
    "# Perform standard imports\n",
    "import spacy\n",
    "\n",
    "# Load English Language Model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Input string\n",
    "string = \"This is the first sentence. This is the second sentence. This is the third sentence.\"\n",
    "\n",
    "# Process the string with spaCy NLP pipeline\n",
    "doc = nlp(string)\n",
    "\n",
    "# Loop through the sentences and print them\n",
    "for sent in doc.sents:\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the first sentence\n",
      "This is the second sentence\n",
      "This is the third sentence\n"
     ]
    }
   ],
   "source": [
    "# Input string\n",
    "string = \"This is the first sentence. This is the second sentence. This is the third sentence.\"\n",
    "\n",
    "# Split the string into sentences using the period as a delimiter\n",
    "sentences = string.split('.')\n",
    "\n",
    "# Remove any leading/trailing whitespace and filter out empty sentences\n",
    "sentences = [sentence.strip() for sentence in sentences if sentence.strip()]\n",
    "\n",
    "# Print each sentence\n",
    "for sentence in sentences:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/sangvu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')  # Download the correct 'punkt' resource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/sangvu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/Users/sangvu/nltk_data'\n    - '/Users/sangvu/miniconda3/envs/llm/nltk_data'\n    - '/Users/sangvu/miniconda3/envs/llm/share/nltk_data'\n    - '/Users/sangvu/miniconda3/envs/llm/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - '/your/custom/nltk_data/path'\n    - '/Users/sangvu/miniconda3/envs/llm/lib/nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTokenization involves splitting text into smaller units or tokens, such as words or phrases. Special cases like contractions (e.g., don\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt) or possessive forms (e.g., John\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms book) require careful handling.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Basic tokenization\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m tokens \u001b[38;5;241m=\u001b[39m \u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBasic Tokenization:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(tokens)\n",
      "File \u001b[0;32m~/miniconda3/envs/llm/lib/python3.10/site-packages/nltk/tokenize/__init__.py:142\u001b[0m, in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mword_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m, preserve_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    128\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;124;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;124;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 142\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m    144\u001b[0m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[1;32m    145\u001b[0m     ]\n",
      "File \u001b[0;32m~/miniconda3/envs/llm/lib/python3.10/site-packages/nltk/tokenize/__init__.py:119\u001b[0m, in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msent_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    110\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;124;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;124;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 119\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43m_get_punkt_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
      "File \u001b[0;32m~/miniconda3/envs/llm/lib/python3.10/site-packages/nltk/tokenize/__init__.py:105\u001b[0m, in \u001b[0;36m_get_punkt_tokenizer\u001b[0;34m(language)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mlru_cache\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_punkt_tokenizer\u001b[39m(language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     98\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;124;03m    A constructor for the PunktTokenizer that utilizes\u001b[39;00m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;124;03m    a lru cache for performance.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;124;03m    :type language: str\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPunktTokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/llm/lib/python3.10/site-packages/nltk/tokenize/punkt.py:1744\u001b[0m, in \u001b[0;36mPunktTokenizer.__init__\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m   1743\u001b[0m     PunktSentenceTokenizer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m-> 1744\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_lang\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/llm/lib/python3.10/site-packages/nltk/tokenize/punkt.py:1749\u001b[0m, in \u001b[0;36mPunktTokenizer.load_lang\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_lang\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m   1747\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m find\n\u001b[0;32m-> 1749\u001b[0m     lang_dir \u001b[38;5;241m=\u001b[39m \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtokenizers/punkt_tab/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlang\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1750\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_params \u001b[38;5;241m=\u001b[39m load_punkt_params(lang_dir)\n\u001b[1;32m   1751\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lang \u001b[38;5;241m=\u001b[39m lang\n",
      "File \u001b[0;32m~/miniconda3/envs/llm/lib/python3.10/site-packages/nltk/data.py:579\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    577\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[1;32m    578\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 579\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/Users/sangvu/nltk_data'\n    - '/Users/sangvu/miniconda3/envs/llm/nltk_data'\n    - '/Users/sangvu/miniconda3/envs/llm/share/nltk_data'\n    - '/Users/sangvu/miniconda3/envs/llm/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - '/your/custom/nltk_data/path'\n    - '/Users/sangvu/miniconda3/envs/llm/lib/nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, regexp_tokenize\n",
    "\n",
    "# Download the 'punkt' tokenizer model \n",
    "nltk.download('punkt')\n",
    "\n",
    "# Example text\n",
    "text = \"Tokenization involves splitting text into smaller units or tokens, such as words or phrases. Special cases like contractions (e.g., don't) or possessive forms (e.g., John's book) require careful handling.\"\n",
    "\n",
    "# Basic tokenization\n",
    "tokens = word_tokenize(text)\n",
    "print(\"Basic Tokenization:\")\n",
    "print(tokens)\n",
    "\n",
    "# Handling contractions and possessives with regex\n",
    "# This regex splits on spaces but keeps contractions and possessives together\n",
    "custom_tokenizer = r\"\\w+\\'\\w+|\\w+\"\n",
    "custom_tokens = regexp_tokenize(text, custom_tokenizer)\n",
    "\n",
    "print(\"\\nCustom Tokenization with handling contractions and possessive forms:\")\n",
    "print(custom_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/sangvu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/sangvu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/sangvu/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence Segmentation:\n",
      "An NLP pipeline processes raw text.\n",
      "It organizes the text in a way that computers can understand.\n",
      "This helps with tasks like figuring out emotions in text, translating languages, and answering questions.\n",
      "\n",
      "Tokenization:\n",
      "['An', 'NLP', 'pipeline', 'processes', 'raw', 'text', '.', 'It', 'organizes', 'the', 'text', 'in', 'a', 'way', 'that', 'computers', 'can', 'understand', '.', 'This', 'helps', 'with', 'tasks', 'like', 'figuring', 'out', 'emotions', 'in', 'text', ',', 'translating', 'languages', ',', 'and', 'answering', 'questions', '.']\n",
      "\n",
      "Text Normalization (Lowercasing):\n",
      "['an', 'nlp', 'pipeline', 'processes', 'raw', 'text', '.', 'it', 'organizes', 'the', 'text', 'in', 'a', 'way', 'that', 'computers', 'can', 'understand', '.', 'this', 'helps', 'with', 'tasks', 'like', 'figuring', 'out', 'emotions', 'in', 'text', ',', 'translating', 'languages', ',', 'and', 'answering', 'questions', '.']\n",
      "\n",
      "Stopword Removal:\n",
      "['nlp', 'pipeline', 'processes', 'raw', 'text', '.', 'organizes', 'text', 'way', 'computers', 'understand', '.', 'helps', 'tasks', 'like', 'figuring', 'emotions', 'text', ',', 'translating', 'languages', ',', 'answering', 'questions', '.']\n",
      "\n",
      "Text Cleaning (Removing punctuation):\n",
      "['nlp', 'pipeline', 'processes', 'raw', 'text', '', 'organizes', 'text', 'way', 'computers', 'understand', '', 'helps', 'tasks', 'like', 'figuring', 'emotions', 'text', '', 'translating', 'languages', '', 'answering', 'questions', '']\n",
      "\n",
      "Stemming:\n",
      "['nlp', 'pipelin', 'process', 'raw', 'text', '', 'organ', 'text', 'way', 'comput', 'understand', '', 'help', 'task', 'like', 'figur', 'emot', 'text', '', 'translat', 'languag', '', 'answer', 'question', '']\n",
      "\n",
      "Lemmatization:\n",
      "['nlp', 'pipeline', 'process', 'raw', 'text', '', 'organizes', 'text', 'way', 'computer', 'understand', '', 'help', 'task', 'like', 'figuring', 'emotion', 'text', '', 'translating', 'language', '', 'answering', 'question', '']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Load spaCy English model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Example text\n",
    "text = \"An NLP pipeline processes raw text. It organizes the text in a way that computers can understand. This helps with tasks like figuring out emotions in text, translating languages, and answering questions.\"\n",
    "\n",
    "# Step 1: Sentence Segmentation (using spaCy)\n",
    "doc = nlp(text)\n",
    "sentences = [sent.text for sent in doc.sents]\n",
    "print(\"Sentence Segmentation:\")\n",
    "for sentence in sentences:\n",
    "    print(sentence)\n",
    "\n",
    "# Step 2: Tokenization (using spaCy)\n",
    "tokens = [token.text for token in doc]\n",
    "print(\"\\nTokenization:\")\n",
    "print(tokens)\n",
    "\n",
    "# Step 3: Text Normalization (Lowercasing)\n",
    "normalized_tokens = [token.lower() for token in tokens]\n",
    "print(\"\\nText Normalization (Lowercasing):\")\n",
    "print(normalized_tokens)\n",
    "\n",
    "# Step 4: Stopword Removal\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_tokens = [token for token in normalized_tokens if token not in stop_words]\n",
    "print(\"\\nStopword Removal:\")\n",
    "print(filtered_tokens)\n",
    "\n",
    "# Step 5: Text Cleaning (Removing punctuation)\n",
    "translator = str.maketrans('', '', string.punctuation)\n",
    "cleaned_tokens = [token.translate(translator) for token in filtered_tokens]\n",
    "print(\"\\nText Cleaning (Removing punctuation):\")\n",
    "print(cleaned_tokens)\n",
    "\n",
    "# Step 6: Stemming and Lemmatization\n",
    "stemmer = PorterStemmer()\n",
    "stemmed_tokens = [stemmer.stem(token) for token in cleaned_tokens]\n",
    "print(\"\\nStemming:\")\n",
    "print(stemmed_tokens)\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_tokens = [lemmatizer.lemmatize(token) for token in cleaned_tokens]\n",
    "print(\"\\nLemmatization:\")\n",
    "print(lemmatized_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/sangvu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')  # This should download the correct 'punkt' model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
